{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c60cd07",
   "metadata": {
    "id": "4c60cd07"
   },
   "source": [
    "## Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced821f",
   "metadata": {
    "id": "0ced821f"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import itertools\n",
    "import functools\n",
    "import numpy as np\n",
    "from six import StringIO\n",
    "from random import sample, randint\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tkinter import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2c0ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5f2c0ef",
    "outputId": "795f7d06-3712-4891-b318-57380b608f10"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # True 表示 GPU 可用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc945b",
   "metadata": {
    "id": "13bc945b"
   },
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e9bb78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67e9bb78",
    "outputId": "d863e7bc-069e-47e5-ed76-4f350b2743bd"
   },
   "outputs": [],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--device\", type=str, default=device)          #是否用CUDA\n",
    "\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=0.001)  # 学习率\n",
    "parser.add_argument(\"--gamma\", type=float, default=0.99)           # 经验折扣率\n",
    "parser.add_argument(\"--epochs\", type=int, default=10000)              # 迭代多少局数\n",
    "\n",
    "parser.add_argument(\"--buffer_size\", type=int, default=10000)      # replaybuffer大小\n",
    "parser.add_argument(\"--batch_size\", type=int, default=128)         # batchsize大小\n",
    "\n",
    "parser.add_argument(\"--pre_train_model\", type=str, default=None)   # 是否加载预训练模型\n",
    "\n",
    "parser.add_argument(\"--use_nature_dqn\", type=bool, default=True)   # 是否采用nature dqn\n",
    "parser.add_argument(\"--target_update_freq\", type=int, default=250) # 如果采用nature dqn，target模型更新频率\n",
    "\n",
    "parser.add_argument(\"--epsilon\", type=float, default=0.999)          # 探索epsilon取值\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6383aba0",
   "metadata": {
    "id": "6383aba0"
   },
   "source": [
    "## Game Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308ae16",
   "metadata": {
    "id": "1308ae16"
   },
   "outputs": [],
   "source": [
    "# Raise the illegal movement when agent does\n",
    "class IllegalMove(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Game2048:\n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.matrix = np.zeros((self.size, self.size), dtype=int)\n",
    "        self.score = 0\n",
    "        self.no_change_count = 0\n",
    "        self.no_change_threshold = 2\n",
    "#         self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"初始化游戏并在两个随机位置添加初始数字\"\"\"\n",
    "        self.new_game()\n",
    "        self.add_tile()\n",
    "        self.add_tile()\n",
    "        self.score = 0\n",
    "\n",
    "    def new_game(self):\n",
    "        \"\"\"创建一个空的矩阵\"\"\"\n",
    "        self.matrix = np.zeros((self.size, self.size), dtype=int)\n",
    "\n",
    "    def add_tile(self):\n",
    "        \"\"\"在矩阵中随机添加一个数字（2或4），使用最小值位置策略\"\"\"\n",
    "        empty_cells, min_value, min_pos = self.find_empty_and_min()\n",
    "        if len(empty_cells) == 0:\n",
    "            return self.matrix\n",
    "\n",
    "        # 找到离最小数字最近的空格子\n",
    "        if np.array_equal(min_pos, (-1, -1)) or min_value != 2:\n",
    "            index_pair = empty_cells[np.random.randint(len(empty_cells))]\n",
    "        else:\n",
    "            # 找到离最小数字最近的空格子\n",
    "\n",
    "            distances = np.array([self.distance(pos, min_pos) for pos in empty_cells])\n",
    "            index_pair = empty_cells[np.argmin(distances)]\n",
    "\n",
    "        if min_value == 2:\n",
    "            ran = np.random.random()  # 确保 ran 是标量\n",
    "        else:\n",
    "            ran = np.random.random() * int(math.log(max(self.score, 1), 2))  # 生成一个标量\n",
    "\n",
    "        # 生成2或4\n",
    "        new_value = 2 if ran < 0.9 else 4\n",
    "\n",
    "        self.matrix[index_pair[0], index_pair[1]] = new_value\n",
    "\n",
    "\n",
    "    def distance(self, pos1, pos2):\n",
    "        \"\"\"计算两个位置的曼哈顿距离\"\"\"\n",
    "        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])\n",
    "\n",
    "    def find_empty_and_min(self, flag='all'):\n",
    "        \"\"\"查找空格子和最小值的位置\"\"\"\n",
    "        empty_cells = np.argwhere(self.matrix == 0)\n",
    "        non_zero_elements = self.matrix[self.matrix != 0]\n",
    "\n",
    "        if non_zero_elements.size > 0:\n",
    "            min_value = np.min(non_zero_elements)\n",
    "            min_pos = np.argwhere(self.matrix == min_value)[0]\n",
    "        else:\n",
    "            min_value, min_pos = float('inf'), (-1, -1)\n",
    "\n",
    "        if flag == 'empty':\n",
    "            return empty_cells\n",
    "        elif flag == 'min':\n",
    "            return min_value, min_pos\n",
    "        elif flag == 'all':\n",
    "            return empty_cells, min_value, min_pos\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid flag value: {flag}. Must be 'empty', 'min', or 'all'.\")\n",
    "\n",
    "    def slide_and_combine(self, row):\n",
    "        \"\"\"将一行的数字先滑动，然后合并，再次滑动\"\"\"\n",
    "        # 移除0并将数字靠一边\n",
    "        non_zero = row[row != 0]\n",
    "        new_row = np.zeros_like(row)\n",
    "\n",
    "        # 合并相邻相同的数字\n",
    "        skip = False\n",
    "        idx = 0\n",
    "        for i in range(len(non_zero)):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            if i + 1 < len(non_zero) and non_zero[i] == non_zero[i + 1]:\n",
    "                new_row[idx] = 2 * non_zero[i]\n",
    "                self.score += new_row[idx]\n",
    "                skip = True\n",
    "            else:\n",
    "                new_row[idx] = non_zero[i]\n",
    "            idx += 1\n",
    "\n",
    "        return new_row\n",
    "\n",
    "    def move_left(self):\n",
    "        \"\"\"左滑操作\"\"\"\n",
    "        for i in range(self.size):\n",
    "            self.matrix[i] = self.slide_and_combine(self.matrix[i])\n",
    "\n",
    "    def move_right(self):\n",
    "        \"\"\"右滑操作\"\"\"\n",
    "        for i in range(self.size):\n",
    "            self.matrix[i] = np.flip(self.slide_and_combine(np.flip(self.matrix[i])))\n",
    "\n",
    "    def move_up(self):\n",
    "        \"\"\"上滑操作\"\"\"\n",
    "        self.matrix = np.transpose(self.matrix)\n",
    "        self.move_left()  # 上滑等效于左滑转置\n",
    "        self.matrix = np.transpose(self.matrix)\n",
    "\n",
    "    def move_down(self):\n",
    "        \"\"\"下滑操作\"\"\"\n",
    "        self.matrix = np.transpose(self.matrix)\n",
    "        self.move_right()  # 下滑等效于右滑转置\n",
    "        self.matrix = np.transpose(self.matrix)\n",
    "\n",
    "\n",
    "\n",
    "    def move(self, direction):\n",
    "\n",
    "        # 保存执行动作前的分数\n",
    "        previous_score = self.score\n",
    "        previous_matrix = self.matrix.copy()\n",
    "\n",
    "        if direction == 0:  # 上\n",
    "            self.move_up()\n",
    "        elif direction == 1:  # 下\n",
    "            self.move_down()\n",
    "        elif direction == 2:  # 左\n",
    "            self.move_left()\n",
    "        elif direction == 3:  # 右\n",
    "            self.move_right()\n",
    "\n",
    "        # Raise exception if movement is illegal\n",
    "        if np.array_equal(self.matrix, previous_matrix):\n",
    "            raise IllegalMove\n",
    "\n",
    "        if np.array_equal(self.matrix, previous_matrix):\n",
    "            self.no_change_count += 1  # 增加未变化计数\n",
    "        else:\n",
    "            self.no_change_count = 0  # 重置计数\n",
    "\n",
    "        # 判断是否达到非法动作的阈值\n",
    "        if self.score > 100:\n",
    "          self.no_change_threshold = 2\n",
    "\n",
    "        if self.no_change_count >= self.no_change_threshold:\n",
    "            raise IllegalMove\n",
    "\n",
    "            #这里有个问题在于有时候我们需要等一回合会增加新的tile，而不是直接结束，增加一个counter，连续两次视为illegal\n",
    "\n",
    "        #calculate the score\n",
    "        reward = self.score - previous_score\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def is_game_over(self):\n",
    "        \"\"\"判断游戏是否结束，返回 True 表示游戏结束，False 表示游戏未结束\"\"\"\n",
    "        # 检查是否有空格子\n",
    "        if np.any(self.matrix == 0):\n",
    "            return False\n",
    "\n",
    "        # 检查相邻元素是否可以合并\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size - 1):\n",
    "                if self.matrix[i, j] == self.matrix[i, j + 1] or self.matrix[j, i] == self.matrix[j + 1, i]:\n",
    "                    return False\n",
    "\n",
    "        # 如果没有可合并的格子，游戏结束\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea47d43",
   "metadata": {
    "id": "fea47d43"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GameGrid(Game2048, Frame):\n",
    "    def __init__(self, master=None):\n",
    "        Game2048.__init__(self)\n",
    "        self.reset()\n",
    "        self.set_illegal_move_reward(-100)\n",
    "\n",
    "\n",
    "    def set_illegal_move_reward(self,reward):\n",
    "        self.illegal_move_reward = reward\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "\n",
    "        try:\n",
    "            # 执行动作（0=上, 1=下, 2=左, 3=右）\n",
    "            reward = self.move(direction=action)\n",
    "\n",
    "            # 判断游戏是否结束\n",
    "            done = self.is_game_over()\n",
    "\n",
    "            # 如果游戏还没结束，添加一个新 tile\n",
    "            if not done:\n",
    "                self.add_tile()\n",
    "\n",
    "        except IllegalMove:\n",
    "            done = True\n",
    "            reward = self.illegal_move_reward\n",
    "\n",
    "        # 返回新的状态、奖励和游戏是否结束\n",
    "        next_state = self.matrix.copy()\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        super().reset()\n",
    "\n",
    "        return self.matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5bc09a",
   "metadata": {
    "id": "ad5bc09a"
   },
   "source": [
    "DQN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a263d272",
   "metadata": {
    "id": "a263d272"
   },
   "source": [
    "目前的问题：\n",
    "- DQN全部为全连接层，不具备卷积能力，对图像捕捉能力不足 -v 通过修改DQN类实现了三个卷积层堆叠\n",
    "- Replay Buffer完全没有，memory靠随机获得\n",
    "- penalty不足，惩罚和跳出机制不足 -v 目前通过重写game2048以及gamegrid的逻辑实现了非法操作的惩罚机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Nzk4B0mYKXy2",
   "metadata": {
    "id": "Nzk4B0mYKXy2"
   },
   "source": [
    "目前的进度Nov05 2024：\n",
    " - nature DQN 尚不完全\n",
    " - replay buffer尚不完全\n",
    " - nature dqn的learn尚不完全\n",
    " - rainbow dqn没有实现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nd4oVT4pIfB_",
   "metadata": {
    "id": "nd4oVT4pIfB_"
   },
   "source": [
    "我记得model会因为误判导致被惩罚，penalty的触发貌似有一丢丢太宽泛了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e1fee0",
   "metadata": {
    "id": "e3e1fee0"
   },
   "source": [
    "0916 Model 因为严重的penalty开始迅速收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SEjEQCxUF0v7",
   "metadata": {
    "id": "SEjEQCxUF0v7"
   },
   "outputs": [],
   "source": [
    "\n",
    "  class ReplayBufferBase:\n",
    "      def __init__(self, buffer_size):\n",
    "          self.buffer_size = buffer_size\n",
    "          self.size = 0\n",
    "          self.pos = 0\n",
    "\n",
    "      def add_transition(self, *args):\n",
    "          raise NotImplementedError(\"This method should be implemented by subclasses.\")\n",
    "\n",
    "      def get_sample(self, sample_size, sample_priority=False):\n",
    "          raise NotImplementedError(\"This method should be implemented by subclasses.\")\n",
    "\n",
    "      def update_priorities(self, indices, priorities):\n",
    "          pass  # 基础类不需要实现优先级更新\n",
    "\n",
    "  class SimpleReplayBuffer(ReplayBufferBase):\n",
    "      def __init__(self, buffer_size):\n",
    "          super().__init__(buffer_size)\n",
    "          self.memory = deque(maxlen=buffer_size)\n",
    "\n",
    "      def add_transition(self, s1, action, s2, done, reward):\n",
    "          self.memory.append((s1, action, s2, done, reward))\n",
    "          self.size = len(self.memory)  # 动态更新大小\n",
    "\n",
    "      def get_sample(self, sample_size, sample_priority=False):\n",
    "          sample = random.sample(self.memory, sample_size)\n",
    "          s1, a, s2, done, r = zip(*sample)\n",
    "          return np.array(s1), np.array(a), np.array(s2), np.array(done), np.array(r)\n",
    "\n",
    "  class OptimizedReplayBuffer(ReplayBufferBase):\n",
    "      def __init__(self, buffer_size, obs_space):\n",
    "          super().__init__(buffer_size)\n",
    "          self.s1 = np.zeros((buffer_size, *obs_space), dtype=np.float32)\n",
    "          self.s2 = np.zeros((buffer_size, *obs_space), dtype=np.float32)\n",
    "          self.a = np.zeros(buffer_size, dtype=np.int32)\n",
    "          self.r = np.zeros(buffer_size, dtype=np.float32)\n",
    "          self.done = np.zeros(buffer_size, dtype=np.float32)\n",
    "\n",
    "      def add_transition(self, s1, action, s2, done, reward):\n",
    "          self.s1[self.pos] = s1\n",
    "          self.a[self.pos] = action\n",
    "          self.s2[self.pos] = s2 if not done else np.zeros_like(s2)\n",
    "          self.done[self.pos] = done\n",
    "          self.r[self.pos] = reward\n",
    "          self.pos = (self.pos + 1) % self.buffer_size\n",
    "          self.size = min(self.size + 1, self.buffer_size)\n",
    "\n",
    "      # def get_sample(self, sample_size, sample_priority=False):\n",
    "\n",
    "      #     # indices = sample(range(0, self.size), sample_size)\n",
    "      #     # return self.s1[indices], self.a[indices], self.s2[indices], self.done[indices], self.r[indices]\n",
    "\n",
    "      def get_sample(self, sample_size):\n",
    "          indices = np.random.choice(self.size, sample_size, replace=False)\n",
    "          return (\n",
    "              self.s1[indices][:, np.newaxis, :, :],  # 添加通道维度\n",
    "              self.a[indices],\n",
    "              self.s2[indices][:, np.newaxis, :, :],  # 添加通道维度\n",
    "              self.done[indices],\n",
    "              self.r[indices]\n",
    "          )\n",
    "\n",
    "\n",
    "      def get_buffer_size(self):\n",
    "        return self.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7830ab7",
   "metadata": {
    "id": "a7830ab7"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size=4):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=2, stride=1)\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=2, stride=1)\n",
    "        self.conv3 = nn.Conv2d(64, 16, kernel_size=2, stride=1)\n",
    "        self.fc1 = nn.Linear(16, action_size)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.fc1(x.view(x.shape[0], -1))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yN_kSFYq41iw",
   "metadata": {
    "id": "yN_kSFYq41iw"
   },
   "source": [
    "写在ver4的留言：改进了学习方法，使得其从原本的单个样本学习变为批量学习，配合ver3主要改进的replaybuffer，以及q_eval的矢量化操作，理论上来说可以使得模型更好更快的学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47331e5d",
   "metadata": {
    "id": "47331e5d"
   },
   "outputs": [],
   "source": [
    "class RLAgent:\n",
    "    def __init__(self, args ,obs_space, action_size):\n",
    "\n",
    "\n",
    "        #self.state_size = np.prod(obs_space)\n",
    "        self.state_size = 1\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.device = torch.device(args.device)\n",
    "\n",
    "        # 将模型和目标模型移动到指定设备（GPU 或 CPU）\n",
    "        self.model = DQN(state_size=self.state_size).to(self.device)\n",
    "        self.target_model = DQN(state_size=self.state_size).to(self.device)  # 用于稳定训练的目标网络\n",
    "\n",
    "        self.lr = args.learning_rate\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "        self.gamma = args.gamma  # 折扣因子\n",
    "        self.epsilon = args.epsilon  # ε-greedy 的探索率\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.replay_buffer = OptimizedReplayBuffer(buffer_size=args.buffer_size,obs_space=obs_space)  # 用于存储经验回放\n",
    "        self.learn_step_counter = 0\n",
    "        self.args = args\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)  # 随机选择动作\n",
    "\n",
    "        # 确保 state 是 NumPy 数组或者 list，然后转换成 GPU 张量\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0).unsqueeze(0)  # 保持空间结构\n",
    "\n",
    "\n",
    "        q_values = self.model(state_tensor)  # 在 GPU 上执行前向传播\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.add_transition(state, action, next_state, done, reward)\n",
    "        # self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    def train(self, batch_size=64):\n",
    "        if self.replay_buffer.get_buffer_size() < batch_size:\n",
    "            return 0\n",
    "\n",
    "        if self.learn_step_counter % args.target_update_freq == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "            self.learn_step_counter += 1\n",
    "\n",
    "\n",
    "        s1, a, s2, done, r = self.replay_buffer.get_sample(batch_size)\n",
    "\n",
    "        s1 = torch.FloatTensor(s1).to(self.device)  # 状态\n",
    "        s2 = torch.FloatTensor(s2).to(self.device)  # 下一状态\n",
    "\n",
    "        r = torch.FloatTensor(r).to(self.device)    # 奖励\n",
    "        a = torch.LongTensor(a).to(self.device)     # 动作\n",
    "        done = torch.FloatTensor(done).to(self.device)  # 是否终止\n",
    "\n",
    "        # 遍历每个样本\n",
    "        next_q_values = self.target_model(s2).detach()  # 使用目标网络\n",
    "        target_q = r + self.gamma * (1 - done) * next_q_values.max(1)[0]  # 终止状态直接为 reward\n",
    "        target_q = target_q.unsqueeze(1)  # 添加维度以匹配 eval_q 的形状\n",
    "\n",
    "        # 计算评估 Q 值\n",
    "        eval_q = self.model(s1).gather(1, a.unsqueeze(1))  # 提取与动作对应的 Q 值\n",
    "\n",
    "        # 计算损失并更新模型\n",
    "        loss = nn.MSELoss()(eval_q, target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AFZcAfbN8T06",
   "metadata": {
    "id": "AFZcAfbN8T06"
   },
   "source": [
    "目前来看，replay buffer的意义在于，我可以利用numpy更快的sample，反正目前用这个optimized的这个buffer好了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kWMKc3zgR95k",
   "metadata": {
    "id": "kWMKc3zgR95k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855bca59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "855bca59",
    "outputId": "0b3bd6e4-34e1-4609-aa64-5c56081b72be"
   },
   "outputs": [],
   "source": [
    "obs_space = (4,4)\n",
    "action_size = 4\n",
    "\n",
    "agent = RLAgent(args, obs_space=obs_space, action_size=action_size)  # RLAgent 的初始化，状态和动作空间设置\n",
    "env = GameGrid()  # 游戏环境\n",
    "all_rewards = []\n",
    "all_losses = []\n",
    "\n",
    "# 定义设备，优先使用 GPU，如果不可用则使用 CPU\n",
    "device = args.device\n",
    "\n",
    "# 假设你的 agent 是 RLAgent 类的实例，确保模型在 GPU 上\n",
    "agent.model.to(device)\n",
    "agent.target_model.to(device)\n",
    "\n",
    "max_reward = 0\n",
    "begin_t = time.time()\n",
    "\n",
    "# 修改训练循环，将数据和模型移动到 GPU\n",
    "for episode in range(args.epochs):  # 假设训练100个episode\n",
    "    state = env.reset()\n",
    "    state = torch.FloatTensor(state).to(device)  # 将状态转为张量并放到 GPU\n",
    "\n",
    "    done = False\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "\n",
    "\n",
    "    while True:\n",
    "\n",
    "        action = agent.act(state)  # 选择动作\n",
    "        next_state, reward, done = env.step(action=action)  # 执行动作并获得下一状态、奖励\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)  # 将下一状态转为张量并放到 GPU\n",
    "        reward = torch.tensor(reward).to(device)  # 将奖励放到 GPU\n",
    "\n",
    "        reward = (reward if not done else reward + env.score)  # 更新奖励\n",
    "\n",
    "        # 存储经验用于经验回放\n",
    "        agent.remember(state.cpu().numpy(), action, reward.cpu().item(), next_state.cpu().numpy(), done)\n",
    "\n",
    "        # 训练Q网络，并返回当前损失\n",
    "        loss = agent.train()\n",
    "        total_loss += loss\n",
    "        steps += 1\n",
    "\n",
    "        # 更新状态\n",
    "        state = next_state\n",
    "        if done:\n",
    "          print(f'Episode {episode}, Total loss: {total_loss},Current Reword: {reward}')\n",
    "          if reward > max_reward:\n",
    "              print(f'Current Max Reword: {reward}')\n",
    "              max_reward = reward\n",
    "              torch.save(agent.model, \"2048.pt\")\n",
    "          break\n",
    "print(\"finish! time cost is {}s\".format(time.time() - begin_t))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb4d5c0",
   "metadata": {
    "id": "dfb4d5c0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "EECS6694",
   "language": "python",
   "name": "eecs6694"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
